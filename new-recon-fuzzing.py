#!/usr/bin/env python3
"""
new-recon-fuzzing.py

Given a CSV generated by new-recon.py (and optionally enriched by new-recon-ainotes.py),
this script finds hosts whose AI Notes mention 401/403/404 responses, runs ffuf dir
bruteforcing, and appends a concise summary of the ffuf findings back into the Notes
column (written to a new CSV to preserve the original data).
"""
import argparse
import csv
import json
import os
import re
import subprocess
import sys
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Dict, List, Optional, Sequence, Set, Tuple
from urllib.parse import urlparse
import tempfile


BASE_COLUMNS = ["DNS", "IP / Hosting Provider", "Ports", "Nuclei", "Notes"]
STATUS_PATTERN = re.compile(r"\b(401|403|404)\b")
HOST_TOKEN_RE = re.compile(
    r"(?:\[[0-9A-Fa-f:]+\]|"
    r"(?:[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)+)|"
    r"(?:\d{1,3}\.){3}\d{1,3})(?::\d{1,5})?"
)
IP_RE = re.compile(r"(?:\d{1,3}\.){3}\d{1,3}")


@dataclass(frozen=True)
class TargetHost:
    host: str
    scheme: str
    port: int
    baseline_status: Optional[int]
    row_indexes: Tuple[int, ...]


@dataclass
class FfufFinding:
    top_redirect: Optional[str]
    top_redirect_status: Optional[int]
    first_ok_path: Optional[str]
    auth_hit: Optional[Tuple[int, str]]
    out_of_scope_hosts: Set[str]

    def empty(self) -> bool:
        return (
            self.top_redirect is None
            and self.first_ok_path is None
            and self.auth_hit is None
            and not self.out_of_scope_hosts
        )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run ffuf against hosts that returned 401/403/404 (per AI Notes) and append results to Notes."
    )
    parser.add_argument("input_csv", help="CSV output from new-recon.py / new-recon-ainotes.py")
    parser.add_argument(
        "-o",
        "--output-csv",
        help="Output CSV path (defaults to new-recon-<domain>_output_ffuf.csv derived from input).",
    )
    parser.add_argument(
        "--wordlist",
        default="./SecLists/Discovery/Web-Content/common.txt",
        help="Path to ffuf wordlist (default ./SecLists/Discovery/Web-Content/common.txt)",
    )
    parser.add_argument(
        "--match-status",
        default="200,301,302,401,403,404",
        help="Comma-separated ffuf -mc codes (default: 200,301,302,401,403,404)",
    )
    parser.add_argument(
        "--threads",
        type=int,
        default=40,
        help="ffuf -t threads (default 40)",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=10,
        help="ffuf -timeout seconds (default 10)",
    )
    parser.add_argument(
        "--max-results-per-status",
        type=int,
        default=0,
        help="Limit ffuf results processed per status (0 = unlimited).",
    )
    parser.add_argument(
        "--ffuf-binary",
        default="ffuf",
        help="ffuf executable name/path (default 'ffuf').",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Detect targets but skip ffuf execution (useful for quick validation).",
    )
    return parser.parse_args()


def read_csv_rows(path: str) -> Tuple[List[Dict[str, str]], List[str]]:
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames or []
        missing = [col for col in BASE_COLUMNS if col not in fieldnames]
        if missing:
            raise SystemExit(f"[!] Missing required column(s): {', '.join(missing)}")
        rows = list(reader)
    return rows, fieldnames


def sanitize_host_token(token: str) -> str:
    token = token.strip()
    if token.startswith("[") and token.endswith("]"):
        token = token[1:-1]
    return token


NOTE_PORT_SUFFIX = re.compile(r":\d{1,5}$")


def strip_port_suffix(token: str) -> str:
    token = token.strip()
    if token.startswith("[") and "]" in token:
        prefix, _, remainder = token.partition("]")
        if remainder.startswith(":"):
            return prefix + "]"
        return token
    return NOTE_PORT_SUFFIX.sub("", token)


def split_hosts(cell: str) -> List[str]:
    if not cell:
        return []
    tokens = re.split(r"[,\s;]+", cell.strip())
    clean = []
    seen = set()
    for tok in tokens:
        tok = sanitize_host_token(tok)
        if not tok or tok.lower() == "none":
            continue
        if tok not in seen:
            clean.append(tok)
            seen.add(tok)
    return clean


def extract_ip_from_provider_cell(cell: str) -> Optional[str]:
    if not cell:
        return None
    match = IP_RE.search(cell)
    return match.group(0) if match else None


def parse_ports_cell(cell: str) -> List[int]:
    if not cell:
        return []
    parts = re.split(r"[,\s]+", cell)
    ports: List[int] = []
    for part in parts:
        part = part.strip()
        if not part:
            continue
        match = re.search(r"tcp/(\d+)", part, re.IGNORECASE)
        if match:
            try:
                ports.append(int(match.group(1)))
            except ValueError:
                continue
    return ports


def choose_scheme_and_port(ports: Sequence[int]) -> Tuple[str, int]:
    ports_set = set(ports)
    https_pref = [443, 8443, 9443, 10443]
    http_pref = [80, 8080, 8000, 8888]
    for candidate in https_pref:
        if candidate in ports_set:
            return "https", candidate
    for candidate in http_pref:
        if candidate in ports_set:
            return "http", candidate
    return "https", 443


def derive_domain_from_filename(path: str) -> Optional[str]:
    basename = os.path.basename(path)
    match = re.match(r"new-recon-([^_]+)_output", basename)
    if match:
        return match.group(1)
    return None


def derive_output_path(input_path: str, provided: Optional[str]) -> str:
    if provided:
        return provided
    domain = derive_domain_from_filename(input_path)
    if domain:
        base_dir = os.path.dirname(input_path)
        return os.path.join(base_dir, f"new-recon-{domain}_output_ffuf.csv")
    return f"{os.path.splitext(input_path)[0]}_ffuf.csv"


def normalize_host(host: str) -> str:
    host = sanitize_host_token(host.strip()).lower()
    if host.endswith("."):
        host = host[:-1]
    return host


def host_for_url(host: str) -> str:
    host = sanitize_host_token(host)
    if ":" in host and not host.startswith("["):
        return f"[{host}]"
    return host


def describe_target_host(host: str, scheme: str, port: int) -> str:
    default_port = 443 if scheme == "https" else 80
    if port and port != default_port:
        return f"{host}:{port}"
    return host


def collect_known_hosts(rows: Sequence[Dict[str, str]]) -> Set[str]:
    known: Set[str] = set()
    for row in rows:
        for host in split_hosts(row.get("DNS", "")):
            known.add(normalize_host(host))
        ip = extract_ip_from_provider_cell(row.get("IP / Hosting Provider", ""))
        if ip:
            known.add(normalize_host(ip))
    return known


def hosts_to_target(
    rows: Sequence[Dict[str, str]]
) -> List[TargetHost]:
    host_to_rows: Dict[str, List[int]] = defaultdict(list)
    targets: Dict[str, Dict[str, Optional[str]]] = {}
    for idx, row in enumerate(rows):
        dns_hosts = split_hosts(row.get("DNS", ""))
        fallback_ip = extract_ip_from_provider_cell(row.get("IP / Hosting Provider", ""))
        if fallback_ip and fallback_ip not in dns_hosts:
            dns_hosts.append(fallback_ip)
        if not dns_hosts:
            continue
        notes = row.get("Notes", "") or ""
        status_match = STATUS_PATTERN.search(notes)
        if not status_match:
            continue
        baseline_status = int(status_match.group(1))
        ports = parse_ports_cell(row.get("Ports", ""))
        scheme, port = choose_scheme_and_port(ports)
        row_hosts_lower = {normalize_host(h): h for h in dns_hosts}
        mentioned_hosts = detect_hosts_in_notes(notes, row_hosts_lower)
        if not mentioned_hosts:
            mentioned_hosts = set(dns_hosts)
        for host in mentioned_hosts:
            norm = normalize_host(host)
            host_to_rows[norm].append(idx)
            if norm not in targets:
                targets[norm] = {"host": host, "scheme": scheme, "port": port, "baseline": baseline_status}
            elif targets[norm].get("baseline") is None and baseline_status:
                targets[norm]["baseline"] = baseline_status
    # finalize row indexes per target
    finalized_targets: List[TargetHost] = []
    for norm, target in targets.items():
        indexes = tuple(sorted(set(host_to_rows[norm])))
        finalized_targets.append(
            TargetHost(
                host=target["host"],
                scheme=target["scheme"],
                port=target["port"],
                baseline_status=target.get("baseline"),
                row_indexes=indexes,
            )
        )
    return finalized_targets


def detect_hosts_in_notes(notes: str, row_hosts_lower: Dict[str, str]) -> Set[str]:
    hosts: Set[str] = set()
    fragments = [frag.strip() for frag in notes.split(";") if frag.strip()]
    for frag in fragments:
        if not STATUS_PATTERN.search(frag):
            continue
        matched: Set[str] = set()
        colon_split = frag.split(":", 1)
        head = colon_split[0] if len(colon_split) == 2 else frag
        for token in HOST_TOKEN_RE.findall(head):
            base = strip_port_suffix(token)
            norm = normalize_host(base)
            if norm in row_hosts_lower:
                matched.add(row_hosts_lower[norm])
        # fallback: if no host token matched, but fragment mentions a host substring
        if not matched:
            frag_lower = frag.lower()
            for norm, original in row_hosts_lower.items():
                if norm in frag_lower:
                    matched.add(original)
        hosts.update(matched)
    return hosts


def build_ffuf_command(
    ffuf_bin: str,
    host: str,
    scheme: str,
    port: int,
    wordlist: str,
    match_status: str,
    threads: int,
    timeout: int,
) -> List[str]:
    host_part = host_for_url(host)
    default_port = 443 if scheme == "https" else 80
    if port and port != default_port:
        base_url = f"{scheme}://{host_part}:{port}/FUZZ"
    else:
        base_url = f"{scheme}://{host_part}/FUZZ"
    return [
        ffuf_bin,
        "-u",
        base_url,
        "-w",
        f"{wordlist}:FUZZ",
        "-of",
        "json",
        "-mc",
        match_status,
        "-t",
        str(threads),
        "-timeout",
        str(timeout),
    ]


def run_ffuf(command: Sequence[str]) -> Optional[Dict]:
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
    tmp_path = tmp_file.name
    tmp_file.close()
    cmd = list(command) + ["-o", tmp_path]
    try:
        completed = subprocess.run(
            cmd,
            check=False,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding="utf-8",
        )
    except FileNotFoundError:
        os.unlink(tmp_path)
        raise SystemExit("[!] ffuf not found. Install ffuf or use --ffuf-binary to point to it.")
    if completed.returncode != 0:
        sys.stderr.write(
            f"[!] ffuf exited with {completed.returncode} for {' '.join(command[:3])}\n"
        )
        if completed.stderr:
            sys.stderr.write(completed.stderr + "\n")
        os.unlink(tmp_path)
        return None
    try:
        with open(tmp_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except (json.JSONDecodeError, OSError):
        sys.stderr.write("[!] Failed to parse ffuf JSON output.\n")
        return None
    finally:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass


def normalize_path(path: str) -> str:
    if not path:
        return "/"
    if not path.startswith("/"):
        path = "/" + path
    return path


def _parse_redirect_target(redirect: str, base_host: str) -> Tuple[str, str]:
    if not redirect:
        return base_host, "/"
    if redirect.startswith("//"):
        parsed = urlparse("https:" + redirect)
    elif "://" in redirect:
        parsed = urlparse(redirect)
    else:
        path = normalize_path(redirect)
        parsed = urlparse(f"https://{base_host}{path}")
    dest_host = parsed.netloc or base_host
    if "@" in dest_host:
        dest_host = dest_host.split("@", 1)[-1]
    dest_host = dest_host.split(":")[0] if ":" in dest_host else dest_host
    dest_path = parsed.path or "/"
    if parsed.query:
        dest_path = dest_path.split("?", 1)[0]
    return dest_host or base_host, normalize_path(dest_path)


def analyze_ffuf_results(
    data: Dict,
    base_host: str,
    known_hosts: Set[str],
    baseline_status: Optional[int],
    max_results_per_status: int,
) -> FfufFinding:
    redirect_counter: Counter[str] = Counter()
    status_by_path: Dict[str, int] = {}
    first_ok_path: Optional[str] = None
    auth_hit: Optional[Tuple[int, str]] = None
    out_of_scope: Set[str] = set()
    results = data.get("results") or []
    base_norm = normalize_host(base_host)
    processed_per_status: Dict[int, int] = defaultdict(int)
    for entry in results:
        status = entry.get("status")
        url = entry.get("url") or ""
        redirect = entry.get("redirectlocation") or ""
        if not isinstance(status, int):
            continue
        processed_per_status[status] += 1
        if max_results_per_status > 0 and processed_per_status[status] > max_results_per_status:
            continue
        parsed_url = urlparse(url)
        path = normalize_path((parsed_url.path or "").split("?", 1)[0])
        if status == 200 and not first_ok_path:
            first_ok_path = path
            status_by_path.setdefault(path, status)
            continue
        if status in (301, 302):
            dest_host, dest_path = _parse_redirect_target(redirect, base_host)
            dest_host_norm = normalize_host(dest_host)
            label = dest_path if dest_host_norm == base_norm else f"{dest_host}{dest_path}"
            redirect_counter[label] += 1
            if dest_host_norm != base_norm and dest_host_norm not in known_hosts:
                out_of_scope.add(dest_host)
            continue
        if status in (401, 403):
            if baseline_status is None or status != baseline_status:
                if not auth_hit:
                    auth_hit = (status, path)
            status_by_path.setdefault(path, status)
            continue
        status_by_path.setdefault(path, status)
    top_redirect = None
    top_redirect_status = None
    if redirect_counter:
        top_redirect = max(redirect_counter.items(), key=lambda item: (item[1], item[0]))[0]
        top_redirect_status = status_by_path.get(top_redirect)
    return FfufFinding(
        top_redirect=top_redirect,
        top_redirect_status=top_redirect_status,
        first_ok_path=first_ok_path,
        auth_hit=auth_hit,
        out_of_scope_hosts=out_of_scope,
    )


def format_findings(label: str, finding: FfufFinding) -> str:
    parts: List[str] = []
    if finding.top_redirect:
        status_suffix = f" [{finding.top_redirect_status}]" if finding.top_redirect_status else ""
        parts.append(f"redirect -> {finding.top_redirect}{status_suffix}")
    elif finding.first_ok_path:
        parts.append(f"200 -> {finding.first_ok_path}")
    elif finding.auth_hit:
        code, path = finding.auth_hit
        parts.append(f"{code} -> {path}")
    if finding.out_of_scope_hosts:
        oos = ", ".join(sorted(finding.out_of_scope_hosts))
        parts.append(f"oos -> {oos}")
    if not parts:
        return f"Fuzzing Results - {label}: no ffuf matches"
    return f"Fuzzing Results - {label}: " + " ; ".join(parts)


def append_notes(original: str, addition: str) -> str:
    original = (original or "").strip()
    if not original:
        return addition
    if original.endswith(";"):
        return original + " " + addition
    return original + " ; " + addition


def ensure_wordlist(path: str) -> None:
    if not os.path.isfile(path):
        raise SystemExit(f"[!] Wordlist not found: {path}")


def main() -> None:
    args = parse_args()
    ensure_wordlist(args.wordlist)
    rows, fieldnames = read_csv_rows(args.input_csv)
    known_hosts = collect_known_hosts(rows)
    targets = hosts_to_target(rows)
    if not targets:
        print("[i] No hosts with 401/403/404 identified in Notes; nothing to do.")
        return
    print(f"[i] Identified {len(targets)} host(s) for ffuf.")
    additions_per_row: Dict[int, List[str]] = defaultdict(list)
    for target in targets:
        cmd = build_ffuf_command(
            args.ffuf_binary,
            target.host,
            target.scheme,
            target.port,
            args.wordlist,
            args.match_status,
            args.threads,
            args.timeout,
        )
        if args.dry_run:
            print("[dry-run] " + " ".join(cmd))
            continue
        ffuf_data = run_ffuf(cmd)
        if not ffuf_data:
            continue
        finding = analyze_ffuf_results(
            ffuf_data,
            target.host,
            known_hosts,
            target.baseline_status,
            args.max_results_per_status,
        )
        target_label = describe_target_host(target.host, target.scheme, target.port)
        summary = format_findings(target_label, finding)
        for row_idx in target.row_indexes:
            additions_per_row[row_idx].append(summary)
    if args.dry_run:
        print("[dry-run] Skipping CSV write because --dry-run was used.")
        return
    for idx, additions in additions_per_row.items():
        if not additions:
            continue
        combined = " | ".join(additions)
        rows[idx]["Notes"] = append_notes(rows[idx].get("Notes", ""), combined)
    output_path = derive_output_path(args.input_csv, args.output_csv)
    with open(output_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    print(f"[+] Wrote {output_path}")


if __name__ == "__main__":
    main()
